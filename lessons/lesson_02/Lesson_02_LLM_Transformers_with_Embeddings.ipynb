{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to Large Language Models and Transformers\n",
    "\n",
    "## Course: Development of Agentic AI Systems for Advertising Campaign Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "In this lesson we will explore:\n",
    "1. What Large Language Models (LLMs) are and how they work\n",
    "2. The Transformer architecture and its key components\n",
    "3. Overview of the main available LLM models\n",
    "4. Practical applications in business contexts\n",
    "5. **NEW: Text embeddings and semantic search**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What are Large Language Models\n",
    "\n",
    "Large Language Models are artificial intelligence models trained on enormous amounts of text to understand and generate natural language. Unlike traditional rule-based systems, LLMs learn linguistic patterns directly from data.\n",
    "\n",
    "### Main characteristics:\n",
    "\n",
    "- **Context understanding**: LLMs analyze the meaning of words based on surrounding context\n",
    "- **Coherent generation**: They produce fluid and grammatically correct text\n",
    "- **Multitask**: Can perform various tasks without being reprogrammed\n",
    "- **Few-shot learning**: Learn new tasks with few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in 2017 in the paper \"Attention is All You Need\", represents the foundation of all modern LLMs.\n",
    "\n",
    "### Fundamental Components\n",
    "\n",
    "#### 1. Tokens\n",
    "Tokens are the basic units of processing. Text is divided into tokens that can represent entire words, parts of words, or single characters.\n",
    "\n",
    "**Tokenization example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tiktoken library for tokenization (used by OpenAI)\n",
    "# !pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Create an encoder for GPT-4\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Example text\n",
    "text = \"Analysis of television advertising campaign performance.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"\\nNumber of tokens: {len(tokens)}\")\n",
    "print(f\"\\nToken IDs: {tokens}\")\n",
    "\n",
    "# Decode each token\n",
    "print(\"\\nDecoded tokens:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_text = encoding.decode([token_id])\n",
    "    print(f\"  {i+1}. '{token_text}' (ID: {token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Self-Attention\n",
    "\n",
    "The self-attention mechanism allows the model to consider all words in a sentence simultaneously, assigning different \"attention weights\" to each word based on their contextual relevance.\n",
    "\n",
    "**Conceptual example:**\n",
    "\n",
    "In the sentence: *\"The advertising campaign reached the predicted target\"*\n",
    "\n",
    "- When the model processes \"target\", it pays particular attention to \"campaign\" and \"advertising\"\n",
    "- When processing \"predicted\", it focuses on \"target\" and \"reached\"\n",
    "\n",
    "This mechanism allows the model to capture long-range relationships in text.\n",
    "\n",
    "**Simplified visualization of the mechanism:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simplified example of attention matrix\n",
    "words = ['The', 'campaign', 'reached', 'the', 'target', 'goal']\n",
    "\n",
    "# Simulated attention matrix (normalized)\n",
    "# Each row shows how much a word \"pays attention\" to others\n",
    "attention_matrix = np.array([\n",
    "    [0.1, 0.3, 0.1, 0.1, 0.2, 0.2],  # The\n",
    "    [0.2, 0.3, 0.1, 0.1, 0.1, 0.2],  # campaign\n",
    "    [0.1, 0.2, 0.2, 0.3, 0.1, 0.1],  # reached\n",
    "    [0.1, 0.2, 0.2, 0.2, 0.1, 0.2],  # the\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.2, 0.4],  # target\n",
    "    [0.1, 0.3, 0.1, 0.2, 0.2, 0.1],  # goal\n",
    "])\n",
    "\n",
    "# Visualize the matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_matrix, \n",
    "            xticklabels=words, \n",
    "            yticklabels=words,\n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Example of Attention Matrix (Self-Attention)', fontsize=14, pad=20)\n",
    "plt.xlabel('Words (Key)', fontsize=12)\n",
    "plt.ylabel('Words (Query)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- More intense colors indicate greater attention\")\n",
    "print(\"- 'target' pays a lot of attention to 'goal' (0.4)\")\n",
    "print(\"- 'campaign' is relevant to many other words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Context Window\n",
    "\n",
    "The context window represents the maximum amount of text that a model can process in a single inference. This capability is measured in number of tokens, not words or characters.\n",
    "\n",
    "**Context window comparison:**\n",
    "\n",
    "| Model | Context Window | Approximate Equivalent |\n",
    "|-------|----------------|------------------------|\n",
    "| GPT-3.5 | 4,096 tokens | ~3,000 words |\n",
    "| GPT-4 | 128,000 tokens | ~96,000 words |\n",
    "| Claude 3 | 200,000 tokens | ~150,000 words |\n",
    "\n",
    "**Practical example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how much text fits in different context windows\n",
    "\n",
    "example_document = \"\"\"\n",
    "Quarterly Campaign Report - Q1 2024\n",
    "\n",
    "Executive Summary:\n",
    "The Spring campaign for Brand XYZ achieved significant reach across multiple \n",
    "channels. Total investment of ‚Ç¨500,000 delivered 15.2M impressions with an \n",
    "average frequency of 4.2.\n",
    "\n",
    "Key Performance Indicators:\n",
    "- Overall Reach: 62% (4.1M unique contacts)\n",
    "- Frequency: 4.2\n",
    "- Total GRPs: 260\n",
    "- Average CPP: ‚Ç¨1,923\n",
    "- CPM: ‚Ç¨32.89\n",
    "\n",
    "Channel Distribution:\n",
    "Linear TV: 55% of budget (‚Ç¨275,000)\n",
    "BVOD: 30% of budget (‚Ç¨150,000)\n",
    "Social Media: 15% of budget (‚Ç¨75,000)\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the document\n",
    "tokens = encoding.encode(example_document)\n",
    "num_tokens = len(tokens)\n",
    "num_words = len(example_document.split())\n",
    "\n",
    "print(f\"Document Analysis:\")\n",
    "print(f\"- Number of characters: {len(example_document)}\")\n",
    "print(f\"- Number of words: {num_words}\")\n",
    "print(f\"- Number of tokens: {num_tokens}\")\n",
    "print(f\"- Token/word ratio: {num_tokens/num_words:.2f}\")\n",
    "\n",
    "# Check compatibility with different models\n",
    "print(\"\\nContext Window Compatibility:\")\n",
    "models = {\n",
    "    'GPT-3.5': 4096,\n",
    "    'GPT-4': 8192,\n",
    "    'GPT-4-Turbo': 128000,\n",
    "    'Claude 3': 200000\n",
    "}\n",
    "\n",
    "for model_name, window_size in models.items():\n",
    "    fits = \"‚úì\" if num_tokens < window_size else \"‚úó\"\n",
    "    percentage = (num_tokens / window_size) * 100\n",
    "    print(f\"{fits} {model_name}: {percentage:.2f}% of context window used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Overview of Main Large Language Models\n",
    "\n",
    "### Comparative Table\n",
    "\n",
    "| Feature | GPT-4 | Claude 3 | Llama 3 70B | Mistral Large |\n",
    "|---------|-------|----------|-------------|---------------|\n",
    "| **Type** | Proprietary | Proprietary | Open-source | Hybrid |\n",
    "| **Context** | 128K | 200K | 8K | 32K |\n",
    "| **Parameters** | ~1.7T | ~200B | 70B | ~100B |\n",
    "| **Cost** | $$$ | $$$ | Free* | $$ |\n",
    "| **Deploy** | API | API | Local/Cloud | API/Local |\n",
    "\n",
    "*Self-hosting requires infrastructure\n",
    "\n",
    "### Model Selection Criteria\n",
    "\n",
    "1. **Context window size**: Evaluate typical document length\n",
    "2. **Budget**: Consider cost per token and expected volume\n",
    "3. **Privacy requirements**: For sensitive data, consider on-premise solutions\n",
    "4. **Multilingual capabilities**: Verify support for required languages\n",
    "5. **Latency requirements**: Consider response time needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Practical Application - Campaign Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Campaign Assistant (without LLM, rule-based)\n",
    "\n",
    "class CampaignAssistant:\n",
    "    def __init__(self):\n",
    "        self.campaign_data = {\n",
    "            'reach': 0.58,  # 58%\n",
    "            'frequency': 4.1,\n",
    "            'unique_contacts': 3_800_000,\n",
    "            'grp': 237,\n",
    "            'period': '6 weeks'\n",
    "        }\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if 'reach' in query_lower:\n",
    "            return f\"The campaign reach is {self.campaign_data['reach']*100}% ({self.campaign_data['unique_contacts']:,} unique contacts)\"\n",
    "        \n",
    "        elif 'frequency' in query_lower:\n",
    "            return f\"The average frequency is {self.campaign_data['frequency']}\"\n",
    "        \n",
    "        elif 'grp' in query_lower:\n",
    "            return f\"Total GRPs: {self.campaign_data['grp']}\"\n",
    "        \n",
    "        elif 'period' in query_lower or 'duration' in query_lower:\n",
    "            return f\"The campaign duration is {self.campaign_data['period']}\"\n",
    "        \n",
    "        else:\n",
    "            return \"I can help you with information about reach, frequency, GRP, or campaign period.\"\n",
    "\n",
    "# Test the assistant\n",
    "assistant = CampaignAssistant()\n",
    "\n",
    "queries = [\n",
    "    \"What is the reach of the campaign?\",\n",
    "    \"Tell me the frequency\",\n",
    "    \"How long did the campaign last?\",\n",
    "    \"What about the budget?\"  # Question the assistant cannot answer\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"A: {assistant.process_query(query)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to Text Embeddings üÜï\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are numerical vector representations of text that capture semantic meaning. Words or sentences with similar meanings have similar embedding vectors.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "- **Dense vectors**: Typically 384-1536 dimensions\n",
    "- **Semantic similarity**: Similar texts have similar embeddings\n",
    "- **Use cases**: \n",
    "  - Semantic search\n",
    "  - Document clustering\n",
    "  - Recommendation systems\n",
    "  - Question answering systems\n",
    "\n",
    "### Why are they important for our project?\n",
    "\n",
    "Embeddings will allow us to:\n",
    "1. Search through campaign documentation semantically\n",
    "2. Find similar campaigns based on descriptions\n",
    "3. Build a knowledge base for the agentic system\n",
    "4. Enable more natural query understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install sentence-transformers scikit-learn numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings with Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "# We'll use 'all-MiniLM-L6-v2' which is fast and efficient\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example campaign descriptions\n",
    "campaign_descriptions = [\n",
    "    \"TV advertising campaign targeting adults 25-54 with focus on prime time slots\",\n",
    "    \"Digital video campaign on streaming platforms for young adults 18-34\",\n",
    "    \"Multi-channel campaign combining linear TV and BVOD for brand awareness\",\n",
    "    \"Social media advertising campaign targeting female audience 25-44\",\n",
    "    \"Prime time television campaign for product launch reaching broad audience\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(campaign_descriptions)\n",
    "\n",
    "print(f\"Number of campaigns: {len(campaign_descriptions)}\")\n",
    "print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"\\nEmbedding for first campaign (first 10 dimensions):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar campaigns\n",
    "def find_similar_campaigns(query, campaign_descriptions, embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most similar campaigns to a query using semantic search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        campaign_descriptions: List of campaign description strings\n",
    "        embeddings: Pre-computed embeddings of campaigns\n",
    "        top_k: Number of most similar results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (campaign_description, similarity_score)\n",
    "    \"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top k most similar\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'campaign': campaign_descriptions[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "test_queries = [\n",
    "    \"streaming video advertising for millennials\",\n",
    "    \"television commercials during evening hours\",\n",
    "    \"social advertising for women\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = find_similar_campaigns(query, campaign_descriptions, embeddings, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Similarity: {result['similarity']:.3f}\")\n",
    "        print(f\"   Campaign: {result['campaign']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings with Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce embeddings from 384 dimensions to 2 for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot campaign embeddings\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "           s=200, alpha=0.6, c='steelblue', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, desc in enumerate(campaign_descriptions):\n",
    "    # Truncate long descriptions for readability\n",
    "    label = desc[:40] + '...' if len(desc) > 40 else desc\n",
    "    plt.annotate(f\"C{i+1}\", \n",
    "                xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Campaign Embeddings Visualization (PCA)', fontsize=16, pad=20)\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print legend\n",
    "print(\"\\nCampaign Legend:\")\n",
    "for i, desc in enumerate(campaign_descriptions, 1):\n",
    "    print(f\"C{i}: {desc}\")\n",
    "\n",
    "print(\"\\nNote: Campaigns closer together in the plot have more similar semantic meaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Campaign Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CampaignKnowledgeBase:\n",
    "    \"\"\"\n",
    "    A simple knowledge base for campaign information using embeddings.\n",
    "    This will be useful for our agentic system to retrieve relevant information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, metadata=None):\n",
    "        \"\"\"\n",
    "        Add documents to the knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents\n",
    "            metadata: Optional list of metadata dictionaries for each document\n",
    "        \"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{} for _ in documents])\n",
    "        \n",
    "        # Generate embeddings for all documents\n",
    "        self.embeddings = self.model.encode(self.documents)\n",
    "        \n",
    "        print(f\"Added {len(documents)} documents. Total documents: {len(self.documents)}\")\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Search for relevant documents using semantic similarity.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with document text, metadata, and similarity score\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'similarity': float(similarities[idx])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics about the knowledge base.\"\"\"\n",
    "        return {\n",
    "            'num_documents': len(self.documents),\n",
    "            'embedding_dimensions': self.embeddings.shape[1] if self.embeddings is not None else 0\n",
    "        }\n",
    "\n",
    "# Create knowledge base\n",
    "kb = CampaignKnowledgeBase()\n",
    "\n",
    "# Add campaign documentation\n",
    "campaign_docs = [\n",
    "    \"Reach represents the percentage of unique individuals who saw the advertisement at least once during the campaign period. It is measured as a deduplicated count.\",\n",
    "    \"Frequency indicates the average number of times each person in the target audience was exposed to the advertisement during the campaign.\",\n",
    "    \"GRP (Gross Rating Points) is calculated as Reach √ó Frequency and represents the total weight of the campaign delivery.\",\n",
    "    \"Impacts (impressions) refer to the total number of times the advertisement was displayed. Unlike reach, impacts are additive across time periods.\",\n",
    "    \"BVOD (Broadcaster Video On Demand) refers to streaming video content from traditional broadcasters, measured on both big screens (TV) and small screens (mobile/tablet).\",\n",
    "    \"The target audience can be defined by demographic filters including sex (M/F) and age breaks (03-14, 15-24, 25-34, 35-44, 45-54, 55-64, 65+).\",\n",
    "    \"CPM (Cost Per Mille) is the cost of reaching 1,000 impressions and is calculated as (Total Cost / Total Impacts) √ó 1000.\",\n",
    "    \"Prime time refers to the evening broadcast period from 20:00 to 22:30, typically achieving the highest viewership and impact delivery.\"\n",
    "]\n",
    "\n",
    "# Add metadata for each document\n",
    "metadata = [\n",
    "    {'category': 'metrics', 'topic': 'reach'},\n",
    "    {'category': 'metrics', 'topic': 'frequency'},\n",
    "    {'category': 'metrics', 'topic': 'grp'},\n",
    "    {'category': 'metrics', 'topic': 'impacts'},\n",
    "    {'category': 'channels', 'topic': 'bvod'},\n",
    "    {'category': 'targeting', 'topic': 'demographics'},\n",
    "    {'category': 'metrics', 'topic': 'cost'},\n",
    "    {'category': 'timing', 'topic': 'daypart'}\n",
    "]\n",
    "\n",
    "kb.add_documents(campaign_docs, metadata)\n",
    "\n",
    "# Test the knowledge base\n",
    "print(f\"\\nKnowledge Base Stats: {kb.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Knowledge Base with Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How do I calculate the total campaign weight?\",\n",
    "    \"What is the difference between reach and impressions?\",\n",
    "    \"How can I target young adults in my campaign?\",\n",
    "    \"What are the best time slots for maximum audience?\"\n",
    "]\n",
    "\n",
    "print(\"Testing Campaign Knowledge Base\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìã Query: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    results = kb.search(query, top_k=2)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n  Result {i} (Similarity: {result['similarity']:.3f})\")\n",
    "        print(f\"  Category: {result['metadata'].get('category', 'N/A')}\")\n",
    "        print(f\"  Text: {result['document']}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Application: Enhanced Campaign Assistant with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCampaignAssistant:\n",
    "    \"\"\"\n",
    "    Campaign assistant that combines rule-based responses with semantic search.\n",
    "    This demonstrates how we'll use embeddings in our agentic system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base):\n",
    "        self.kb = knowledge_base\n",
    "        self.campaign_data = {\n",
    "            'reach': 0.58,\n",
    "            'frequency': 4.1,\n",
    "            'unique_contacts': 3_800_000,\n",
    "            'grp': 237,\n",
    "            'impressions': 15_580_000,\n",
    "            'budget': 500_000,\n",
    "            'period': '6 weeks'\n",
    "        }\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        \"\"\"\n",
    "        Process user query by combining data lookup with knowledge base search.\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check if query is asking for specific campaign data\n",
    "        if any(metric in query_lower for metric in ['reach', 'frequency', 'grp', 'impressions', 'budget']):\n",
    "            response = self._get_metric_data(query_lower)\n",
    "        else:\n",
    "            # Use knowledge base for conceptual questions\n",
    "            response = self._search_knowledge_base(query)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _get_metric_data(self, query):\n",
    "        \"\"\"Get specific metric data from campaign.\"\"\"\n",
    "        if 'reach' in query:\n",
    "            return f\"Campaign reach: {self.campaign_data['reach']*100}% ({self.campaign_data['unique_contacts']:,} unique contacts)\"\n",
    "        elif 'frequency' in query:\n",
    "            return f\"Average frequency: {self.campaign_data['frequency']}\"\n",
    "        elif 'grp' in query:\n",
    "            return f\"Total GRPs: {self.campaign_data['grp']}\"\n",
    "        elif 'impressions' in query or 'impacts' in query:\n",
    "            return f\"Total impressions: {self.campaign_data['impressions']:,}\"\n",
    "        elif 'budget' in query:\n",
    "            return f\"Campaign budget: ‚Ç¨{self.campaign_data['budget']:,}\"\n",
    "    \n",
    "    def _search_knowledge_base(self, query):\n",
    "        \"\"\"Search knowledge base for relevant information.\"\"\"\n",
    "        results = self.kb.search(query, top_k=2)\n",
    "        \n",
    "        if not results:\n",
    "            return \"I don't have information about that. Please try rephrasing your question.\"\n",
    "        \n",
    "        # Format response with most relevant result\n",
    "        best_result = results[0]\n",
    "        response = f\"Based on the knowledge base (similarity: {best_result['similarity']:.2f}):\\n\\n\"\n",
    "        response += best_result['document']\n",
    "        \n",
    "        if len(results) > 1 and results[1]['similarity'] > 0.5:\n",
    "            response += f\"\\n\\nAdditionally:\\n{results[1]['document']}\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create enhanced assistant\n",
    "enhanced_assistant = EnhancedCampaignAssistant(kb)\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    \"What was the reach of the campaign?\",\n",
    "    \"How do I calculate GRP?\",\n",
    "    \"Explain the difference between reach and impressions\",\n",
    "    \"What is BVOD?\",\n",
    "    \"Tell me about the campaign frequency\"\n",
    "]\n",
    "\n",
    "print(\"Enhanced Campaign Assistant with Embeddings\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Question: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    response = enhanced_assistant.process_query(query)\n",
    "    print(f\"üí° Answer: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways: Embeddings for Agentic Systems\n",
    "\n",
    "1. **Semantic Understanding**: Embeddings enable our system to understand the meaning of queries, not just keywords\n",
    "\n",
    "2. **Knowledge Retrieval**: We can build a knowledge base that the agent can query intelligently\n",
    "\n",
    "3. **Scalability**: As we add more campaign documentation, embeddings allow efficient semantic search\n",
    "\n",
    "4. **Foundation for RAG**: This is the basis for Retrieval-Augmented Generation, which we'll use in our agentic system\n",
    "\n",
    "### Next Steps in the Course:\n",
    "\n",
    "- Integrate embeddings with LLMs for more sophisticated responses\n",
    "- Build a vector database for efficient large-scale search\n",
    "- Implement RAG (Retrieval-Augmented Generation) patterns\n",
    "- Use embeddings to help agents decide which tools to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Exercises\n",
    "\n",
    "### Exercise 1: Tokenization Analysis\n",
    "\n",
    "**Objective:** Understand how different texts are tokenized.\n",
    "\n",
    "**Task:** \n",
    "1. Take the following advertising campaign text\n",
    "2. Calculate the number of tokens using GPT-4 encoder\n",
    "3. Analyze the token/word ratio\n",
    "4. Determine if the text can be processed by models with different context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1\n",
    "\n",
    "exercise_text = \"\"\"\n",
    "The multi-channel campaign for brand XYZ featured a coordinated implementation\n",
    "across linear television, streaming video, and social media platforms. The flight\n",
    "period extended for 6 consecutive weeks, with a budget allocation of 60% on linear TV,\n",
    "30% on BVOD, and 10% on social media.\n",
    "\n",
    "Preliminary results show:\n",
    "- Overall reach: 58% (3.8M unique contacts)\n",
    "- Average frequency: 4.1\n",
    "- Total GRPs: 237\n",
    "- Average CPP: ‚Ç¨1,250\n",
    "\n",
    "Performance by time slot shows a peak during prime time (20:00-22:30)\n",
    "with 45% of impressions concentrated in this time window.\n",
    "\"\"\"\n",
    "\n",
    "# Your code here:\n",
    "# 1. Tokenize the text\n",
    "# 2. Calculate number of tokens, words, characters\n",
    "# 3. Calculate token/word ratio\n",
    "# 4. Check compatibility with context windows: GPT-3.5 (4K), GPT-4 (8K), Claude (200K)\n",
    "\n",
    "# Solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Assistant Extension\n",
    "\n",
    "**Objective:** Create an enhanced conversational assistant.\n",
    "\n",
    "**Task:**\n",
    "Extend the `CampaignAssistant` class by adding:\n",
    "1. The ability to answer budget questions\n",
    "2. The ability to calculate CPM (Cost Per Thousand)\n",
    "3. The ability to compare two campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "class ExtendedCampaignAssistant(CampaignAssistant):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add budget data\n",
    "        self.campaign_data['budget'] = 150000  # euros\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        # Your code here:\n",
    "        # Extend the method to handle:\n",
    "        # - Budget questions\n",
    "        # - CPM calculation\n",
    "        # - Other metrics\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test your extended assistant\n",
    "# assistant_extended = ExtendedCampaignAssistant()\n",
    "# print(assistant_extended.process_query(\"What is the campaign budget?\"))\n",
    "# print(assistant_extended.process_query(\"What is the CPM?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Model Comparison\n",
    "\n",
    "**Objective:** Understand when to choose one model over another.\n",
    "\n",
    "**Task:**\n",
    "For each of the following scenarios, indicate which LLM model you would recommend and why:\n",
    "\n",
    "1. Analysis of monthly reports of 50 pages each to extract KPIs\n",
    "2. Customer service chatbot with 10,000 requests per day\n",
    "3. Sensitive data analysis system that must remain on-premise\n",
    "4. Creative content generation for multilingual social campaigns\n",
    "5. Quick prototype for client demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3\n",
    "# Write your answers below as comments\n",
    "\n",
    "exercise_3_answers = {\n",
    "    'scenario_1': {\n",
    "        'recommended_model': '',  # Insert model\n",
    "        'rationale': ''  # Explain why\n",
    "    },\n",
    "    'scenario_2': {\n",
    "        'recommended_model': '',\n",
    "        'rationale': ''\n",
    "    },\n",
    "    'scenario_3': {\n",
    "        'recommended_model': '',\n",
    "        'rationale': ''\n",
    "    },\n",
    "    'scenario_4': {\n",
    "        'recommended_model': '',\n",
    "        'rationale': ''\n",
    "    },\n",
    "    'scenario_5': {\n",
    "        'recommended_model': '',\n",
    "        'rationale': ''\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example answer for scenario_1:\n",
    "# exercise_3_answers['scenario_1'] = {\n",
    "#     'recommended_model': 'Claude 3',\n",
    "#     'rationale': 'Context window of 200K tokens allows analyzing very long documents in a single call'\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Building Your Own Knowledge Base üÜï\n",
    "\n",
    "**Objective:** Create a knowledge base for a specific domain.\n",
    "\n",
    "**Task:**\n",
    "1. Create a knowledge base with at least 5 documents about advertising concepts\n",
    "2. Add appropriate metadata for each document\n",
    "3. Test it with 3 different queries\n",
    "4. Analyze the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 4\n",
    "\n",
    "# Create your own knowledge base\n",
    "my_kb = CampaignKnowledgeBase()\n",
    "\n",
    "# Your documents here\n",
    "my_documents = [\n",
    "    # Add your documents\n",
    "]\n",
    "\n",
    "my_metadata = [\n",
    "    # Add your metadata\n",
    "]\n",
    "\n",
    "# Add documents to knowledge base\n",
    "# my_kb.add_documents(my_documents, my_metadata)\n",
    "\n",
    "# Test with queries\n",
    "# test_queries = [\n",
    "#     \"Your query 1\",\n",
    "#     \"Your query 2\",\n",
    "#     \"Your query 3\"\n",
    "# ]\n",
    "\n",
    "# for query in test_queries:\n",
    "#     print(f\"\\nQuery: {query}\")\n",
    "#     results = my_kb.search(query, top_k=2)\n",
    "#     for result in results:\n",
    "#         print(f\"Similarity: {result['similarity']:.3f}\")\n",
    "#         print(f\"Document: {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading Resources:\n",
    "\n",
    "**Core Papers:**\n",
    "- **Original Transformer Paper**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- **Sentence-BERT**: \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\" (Reimers & Gurevych, 2019)\n",
    "\n",
    "**Documentation:**\n",
    "- **OpenAI Documentation**: https://platform.openai.com/docs/\n",
    "- **Anthropic Claude Documentation**: https://docs.anthropic.com/\n",
    "- **Hugging Face NLP Course**: https://huggingface.co/learn/nlp-course/\n",
    "- **Sentence Transformers**: https://www.sbert.net/\n",
    "\n",
    "**Additional Topics:**\n",
    "- Vector databases (Pinecone, Weaviate, ChromaDB)\n",
    "- RAG (Retrieval-Augmented Generation)\n",
    "- Prompt engineering techniques\n",
    "\n",
    "---\n",
    "\n",
    "**End of Lesson 1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
