{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Fundamental Concepts of Agentic AI\n",
    "## Practical Examples and Exercises\n",
    "\n",
    "This notebook contains practical examples and exercises to reinforce the concepts covered in Lesson 3.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between chatbots and autonomous agents\n",
    "- Implement basic agentic patterns\n",
    "- Design and use tools in an agentic system\n",
    "- Practice building simple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q groq google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import groq\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "\n",
    "Configure your API keys for Groq and Google Gemini.\n",
    "\n",
    "**Get your API keys:**\n",
    "- Groq: https://console.groq.com\n",
    "- Gemini: https://aistudio.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here or use environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"your-groq-api-key-here\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your-gemini-api-key-here\")\n",
    "\n",
    "# Initialize clients\n",
    "groq_client = groq.Groq(api_key=GROQ_API_KEY)\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Theory Recap\n",
    "\n",
    "## Chatbot vs Agent\n",
    "\n",
    "**Traditional Chatbot:**\n",
    "- Reactive: responds to user input\n",
    "- No planning capability\n",
    "- Cannot use external tools\n",
    "- Limited memory\n",
    "\n",
    "**Autonomous Agent:**\n",
    "- Proactive: pursues goals\n",
    "- Plans multi-step actions\n",
    "- Uses external tools (APIs, databases, calculators)\n",
    "- Strategic memory management\n",
    "\n",
    "## Agent Execution Cycle\n",
    "\n",
    "```\n",
    "1. PERCEIVE → Receive input\n",
    "2. REASON → Analyze and plan\n",
    "3. ACT → Execute action (use tool)\n",
    "4. OBSERVE → Get result\n",
    "5. EVALUATE → Check if goal reached\n",
    "   ↓ (if not done)\n",
    "   Back to REASON\n",
    "```\n",
    "\n",
    "## ReAct Pattern\n",
    "\n",
    "**Re**asoning + **Act**ing\n",
    "```\n",
    "Thought: \"I need to get campaign data\"\n",
    "Action: call_api(\"get_campaign\", params)\n",
    "Observation: {\"reach\": \"45%\"}\n",
    "Thought: \"Now I can answer\"\n",
    "Final Answer: \"The reach is 45%\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Example 1 - Simulating ReAct Pattern\n",
    "\n",
    "Let's manually implement a simple ReAct cycle to understand how agents think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleReActAgent:\n",
    "    \"\"\"\n",
    "    A simple implementation of ReAct pattern for educational purposes.\n",
    "    This simulates how an agent alternates between reasoning and acting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        self.max_iterations = 5\n",
    "        \n",
    "    def perceive(self, user_input: str):\n",
    "        \"\"\"Receive input from user\"\"\"\n",
    "        print(f\"\\n[PERCEIVE] User says: {user_input}\")\n",
    "        return user_input\n",
    "    \n",
    "    def reason(self, context: str, iteration: int) -> Dict[str, Any]:\n",
    "        \"\"\"Decide what action to take\"\"\"\n",
    "        print(f\"\\n[REASON - Iteration {iteration}] Thinking about what to do...\")\n",
    "        \n",
    "        # Simulate reasoning (in real agent, this would be LLM call)\n",
    "        if \"campaign\" in context.lower() and \"reach\" in context.lower():\n",
    "            thought = \"User wants reach data for a campaign. I need to call the API.\"\n",
    "            action = \"get_campaign_data\"\n",
    "            params = {\"campaign_id\": \"autumn_2024\", \"metric\": \"reach\"}\n",
    "        elif \"compare\" in context.lower():\n",
    "            thought = \"User wants to compare campaigns. I need to get data for both.\"\n",
    "            action = \"compare_campaigns\"\n",
    "            params = {\"campaigns\": [\"autumn_2024\", \"winter_2024\"]}\n",
    "        else:\n",
    "            thought = \"I have enough information to answer.\"\n",
    "            action = \"final_answer\"\n",
    "            params = {}\n",
    "            \n",
    "        print(f\"[REASON] Thought: {thought}\")\n",
    "        return {\"thought\": thought, \"action\": action, \"params\": params}\n",
    "    \n",
    "    def act(self, action: str, params: Dict) -> Any:\n",
    "        \"\"\"Execute the decided action\"\"\"\n",
    "        print(f\"\\n[ACT] Executing: {action} with params: {params}\")\n",
    "        \n",
    "        # Simulate tool execution (in real agent, this would call actual tools)\n",
    "        if action == \"get_campaign_data\":\n",
    "            # Simulate API response\n",
    "            result = {\"reach\": \"48%\", \"frequency\": 3.8, \"impressions\": 5200000}\n",
    "        elif action == \"compare_campaigns\":\n",
    "            result = {\n",
    "                \"autumn_2024\": {\"reach\": \"48%\"},\n",
    "                \"winter_2024\": {\"reach\": \"52%\"}\n",
    "            }\n",
    "        else:\n",
    "            result = None\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def observe(self, result: Any) -> str:\n",
    "        \"\"\"Process the result of the action\"\"\"\n",
    "        print(f\"\\n[OBSERVE] Result: {result}\")\n",
    "        observation = f\"Tool returned: {json.dumps(result, indent=2)}\"\n",
    "        self.memory.append(observation)\n",
    "        return observation\n",
    "    \n",
    "    def evaluate(self, action: str) -> bool:\n",
    "        \"\"\"Check if we have reached the goal\"\"\"\n",
    "        print(f\"\\n[EVALUATE] Checking if goal is reached...\")\n",
    "        goal_reached = action == \"final_answer\"\n",
    "        print(f\"[EVALUATE] Goal reached: {goal_reached}\")\n",
    "        return goal_reached\n",
    "    \n",
    "    def run(self, user_input: str):\n",
    "        \"\"\"Execute the full ReAct cycle\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING REACT AGENT CYCLE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        context = self.perceive(user_input)\n",
    "        \n",
    "        for iteration in range(1, self.max_iterations + 1):\n",
    "            # Reason\n",
    "            decision = self.reason(context, iteration)\n",
    "            \n",
    "            # Act\n",
    "            result = self.act(decision[\"action\"], decision[\"params\"])\n",
    "            \n",
    "            # Evaluate\n",
    "            if self.evaluate(decision[\"action\"]):\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"AGENT COMPLETED TASK\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "                \n",
    "            # Observe\n",
    "            observation = self.observe(result)\n",
    "            context = context + \"\\n\" + observation\n",
    "            \n",
    "        return self.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simple ReAct agent\n",
    "agent = SimpleReActAgent()\n",
    "agent.run(\"What is the reach of the autumn campaign?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Example 2 - Tool Definition\n",
    "\n",
    "Let's define tools that an agent could use to interact with the TTVAM API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool schemas (how we tell the agent what tools are available)\n",
    "\n",
    "TTVAM_TOOLS = [\n",
    "    {\n",
    "        \"name\": \"get_campaign_performance\",\n",
    "        \"description\": \"\"\"Retrieves performance data for an advertising campaign \n",
    "                          from TTVAM system. Use this when you need metrics like \n",
    "                          reach, frequency, or impressions for a specific campaign.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"spotgate_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The Spotgate code identifying the campaign\"\n",
    "                },\n",
    "                \"period_start\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Start date in YYYY-MM-DD format\"\n",
    "                },\n",
    "                \"period_end\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"End date in YYYY-MM-DD format\"\n",
    "                },\n",
    "                \"target\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"description\": \"Demographic target (optional)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"spotgate_code\", \"period_start\", \"period_end\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate_kpi\",\n",
    "        \"description\": \"\"\"Calculates derived KPIs from campaign data. \n",
    "                          Use this after retrieving raw data to compute \n",
    "                          metrics like CPM, cost per reach point, etc.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"impressions\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Total impressions\"\n",
    "                },\n",
    "                \"reach\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Reach percentage\"\n",
    "                },\n",
    "                \"budget\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Campaign budget in currency\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"impressions\", \"reach\", \"budget\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"compare_campaigns\",\n",
    "        \"description\": \"\"\"Compares performance of multiple campaigns. \n",
    "                          Use this when user asks to compare or benchmark campaigns.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"campaign_ids\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"description\": \"List of campaign Spotgate codes to compare\"\n",
    "                },\n",
    "                \"metrics\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"description\": \"Metrics to compare (reach, frequency, impressions)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"campaign_ids\", \"metrics\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display tool definitions\n",
    "print(\"Available Tools for TTVAM Agent:\\n\")\n",
    "for tool in TTVAM_TOOLS:\n",
    "    print(f\"Tool: {tool['name']}\")\n",
    "    print(f\"Description: {tool['description']}\")\n",
    "    print(f\"Parameters: {list(tool['parameters']['properties'].keys())}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Implementation\n",
    "\n",
    "Now let's implement the actual functions that execute these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaign_performance(spotgate_code: str, period_start: str, \n",
    "                            period_end: str, target: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Simulates retrieving campaign performance data from TTVAM API.\n",
    "    In production, this would make actual API calls.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[TOOL] Calling get_campaign_performance...\")\n",
    "    print(f\"  Spotgate Code: {spotgate_code}\")\n",
    "    print(f\"  Period: {period_start} to {period_end}\")\n",
    "    \n",
    "    # Simulate API response\n",
    "    mock_data = {\n",
    "        \"1234\": {\n",
    "            \"reach\": 48.5,\n",
    "            \"frequency\": 3.8,\n",
    "            \"impressions\": 5200000,\n",
    "            \"campaign_name\": \"Autumn Campaign 2024\"\n",
    "        },\n",
    "        \"5678\": {\n",
    "            \"reach\": 52.3,\n",
    "            \"frequency\": 4.2,\n",
    "            \"impressions\": 6100000,\n",
    "            \"campaign_name\": \"Winter Campaign 2024\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    result = mock_data.get(spotgate_code, {\n",
    "        \"reach\": 45.0,\n",
    "        \"frequency\": 3.5,\n",
    "        \"impressions\": 4800000,\n",
    "        \"campaign_name\": \"Unknown Campaign\"\n",
    "    })\n",
    "    \n",
    "    print(f\"[TOOL] Retrieved data: {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_kpi(impressions: float, reach: float, budget: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculates derived KPIs from campaign metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[TOOL] Calculating KPIs...\")\n",
    "    print(f\"  Impressions: {impressions:,.0f}\")\n",
    "    print(f\"  Reach: {reach}%\")\n",
    "    print(f\"  Budget: €{budget:,.2f}\")\n",
    "    \n",
    "    # Calculate KPIs\n",
    "    cpm = (budget / impressions) * 1000\n",
    "    cost_per_reach_point = budget / reach\n",
    "    \n",
    "    result = {\n",
    "        \"cpm\": round(cpm, 2),\n",
    "        \"cost_per_reach_point\": round(cost_per_reach_point, 2),\n",
    "        \"efficiency_score\": round(reach / (cpm * 10), 2)\n",
    "    }\n",
    "    \n",
    "    print(f\"[TOOL] KPIs calculated: {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def compare_campaigns(campaign_ids: List[str], metrics: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compares performance of multiple campaigns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[TOOL] Comparing campaigns...\")\n",
    "    print(f\"  Campaigns: {campaign_ids}\")\n",
    "    print(f\"  Metrics: {metrics}\")\n",
    "    \n",
    "    # Get data for each campaign\n",
    "    comparison = {}\n",
    "    for campaign_id in campaign_ids:\n",
    "        data = get_campaign_performance(campaign_id, \"2024-01-01\", \"2024-12-31\")\n",
    "        comparison[campaign_id] = {k: v for k, v in data.items() if k in metrics}\n",
    "    \n",
    "    print(f\"[TOOL] Comparison result: {comparison}\")\n",
    "    return comparison\n",
    "\n",
    "\n",
    "# Tool registry - maps tool names to functions\n",
    "TOOL_REGISTRY = {\n",
    "    \"get_campaign_performance\": get_campaign_performance,\n",
    "    \"calculate_kpi\": calculate_kpi,\n",
    "    \"compare_campaigns\": compare_campaigns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tools\n",
    "print(\"Testing TTVAM Tools:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Get campaign performance\n",
    "result1 = get_campaign_performance(\"1234\", \"2024-03-01\", \"2024-03-31\")\n",
    "\n",
    "# Test 2: Calculate KPIs\n",
    "result2 = calculate_kpi(5200000, 48.5, 50000)\n",
    "\n",
    "# Test 3: Compare campaigns\n",
    "result3 = compare_campaigns([\"1234\", \"5678\"], [\"reach\", \"frequency\", \"impressions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Example 3 - Simple Agent with LLM\n",
    "\n",
    "Now let's create a simple agent that uses an LLM (Groq or Gemini) to make decisions about which tools to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_prompt(user_query: str, available_tools: List[Dict], \n",
    "                       conversation_history: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt that instructs the LLM to act as an agent.\n",
    "    \"\"\"\n",
    "    tools_description = \"\\n\".join([\n",
    "        f\"- {tool['name']}: {tool['description']}\"\n",
    "        for tool in available_tools\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an intelligent agent that helps analyze advertising campaigns.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_description}\n",
    "\n",
    "To use a tool, respond with a JSON object in this format:\n",
    "{{\n",
    "  \"thought\": \"your reasoning about what to do\",\n",
    "  \"action\": \"tool_name\",\n",
    "  \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}}\n",
    "}}\n",
    "\n",
    "When you have all the information needed to answer the user's question, respond with:\n",
    "{{\n",
    "  \"thought\": \"I have all the information needed\",\n",
    "  \"action\": \"final_answer\",\n",
    "  \"answer\": \"your complete answer to the user\"\n",
    "}}\n",
    "\n",
    "Conversation history:\n",
    "{conversation_history}\n",
    "\n",
    "User query: {user_query}\n",
    "\n",
    "Respond with JSON only, no other text:\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_agent_groq(user_query: str, max_iterations: int = 5):\n",
    "    \"\"\"\n",
    "    Runs a simple agent using Groq's Llama model.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING SIMPLE AGENT WITH GROQ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nUser Query: {user_query}\\n\")\n",
    "    \n",
    "    conversation_history = \"\"\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"\\n--- Iteration {iteration} ---\")\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = create_agent_prompt(user_query, TTVAM_TOOLS, conversation_history)\n",
    "        \n",
    "        # Get LLM response\n",
    "        try:\n",
    "            response = groq_client.chat.completions.create(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            agent_response = response.choices[0].message.content\n",
    "            print(f\"\\n[AGENT RESPONSE]\\n{agent_response}\")\n",
    "            \n",
    "            # Parse JSON response\n",
    "            try:\n",
    "                decision = json.loads(agent_response)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to extract JSON from response\n",
    "                import re\n",
    "                json_match = re.search(r'\\{.*\\}', agent_response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    decision = json.loads(json_match.group())\n",
    "                else:\n",
    "                    print(\"[ERROR] Could not parse agent response as JSON\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\n[THOUGHT] {decision.get('thought', 'No thought provided')}\")\n",
    "            print(f\"[ACTION] {decision.get('action', 'No action specified')}\")\n",
    "            \n",
    "            # Check if agent wants to provide final answer\n",
    "            if decision.get('action') == 'final_answer':\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"FINAL ANSWER:\")\n",
    "                print(f\"{'='*60}\")\n",
    "                print(decision.get('answer', 'No answer provided'))\n",
    "                break\n",
    "            \n",
    "            # Execute tool\n",
    "            tool_name = decision.get('action')\n",
    "            if tool_name in TOOL_REGISTRY:\n",
    "                tool_function = TOOL_REGISTRY[tool_name]\n",
    "                parameters = decision.get('parameters', {})\n",
    "                \n",
    "                result = tool_function(**parameters)\n",
    "                \n",
    "                # Add to conversation history\n",
    "                conversation_history += f\"\\nAction taken: {tool_name}\\n\"\n",
    "                conversation_history += f\"Result: {json.dumps(result)}\\n\"\n",
    "            else:\n",
    "                print(f\"[ERROR] Unknown tool: {tool_name}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AGENT EXECUTION COMPLETED\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple query\n",
    "run_simple_agent_groq(\"What was the reach of campaign 1234?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Exercises\n",
    "\n",
    "Now it's your turn to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Design a New Tool\n",
    "\n",
    "Design a tool called `generate_report` that creates a summary report for a campaign.\n",
    "\n",
    "**Requirements:**\n",
    "- Tool name: `generate_report`\n",
    "- Description: Should clearly explain what the tool does\n",
    "- Parameters: \n",
    "  - `campaign_id` (string, required)\n",
    "  - `report_type` (string, required): \"executive\" or \"detailed\"\n",
    "  - `include_charts` (boolean, optional)\n",
    "\n",
    "Complete the tool definition below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "generate_report_tool = {\n",
    "    \"name\": \"generate_report\",\n",
    "    \"description\": \"\"\"# TODO: Write a clear description of what this tool does\n",
    "                      # Hint: Explain when an agent should use this tool\n",
    "                      \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            # TODO: Define the parameters here\n",
    "            # Use the TTVAM_TOOLS examples above as reference\n",
    "        },\n",
    "        \"required\": []  # TODO: List required parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test your tool definition\n",
    "print(json.dumps(generate_report_tool, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement the Tool Function\n",
    "\n",
    "Now implement the actual function for the `generate_report` tool.\n",
    "\n",
    "The function should:\n",
    "1. Call `get_campaign_performance` to get the data\n",
    "2. Create a summary based on `report_type`\n",
    "3. Return a formatted report as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "def generate_report(campaign_id: str, report_type: str, include_charts: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Generates a summary report for a campaign.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: Spotgate code of the campaign\n",
    "        report_type: Type of report (\"executive\" or \"detailed\")\n",
    "        include_charts: Whether to include chart recommendations\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the report\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Steps:\n",
    "    # 1. Get campaign data using get_campaign_performance\n",
    "    # 2. Create summary based on report_type\n",
    "    # 3. If report_type == \"executive\", provide brief summary\n",
    "    # 4. If report_type == \"detailed\", provide comprehensive summary\n",
    "    # 5. If include_charts, suggest appropriate charts\n",
    "    # 6. Return report as dictionary\n",
    "    \n",
    "    pass  # Remove this and add your implementation\n",
    "\n",
    "# Test your function\n",
    "# result = generate_report(\"1234\", \"executive\", include_charts=True)\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Build an Agent Prompt\n",
    "\n",
    "Create a prompt for an agent that specializes in comparing campaigns.\n",
    "\n",
    "**Requirements:**\n",
    "- The agent should be an expert in campaign comparison\n",
    "- It should always use the `compare_campaigns` tool when asked to compare\n",
    "- It should provide insights about which campaign performed better and why\n",
    "- Include examples in your prompt (few-shot learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "def create_comparison_agent_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a specialized prompt for a campaign comparison agent.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"# TODO: Create your agent prompt here\n",
    "    \n",
    "    Guidelines:\n",
    "    1. Define the agent's role (campaign comparison expert)\n",
    "    2. List available tools\n",
    "    3. Provide examples of good comparisons (few-shot)\n",
    "    4. Explain the expected output format\n",
    "    5. Include the user query\n",
    "    \n",
    "    Example structure:\n",
    "    - You are a...\n",
    "    - You have access to...\n",
    "    - Example 1: User asks \"Compare X and Y\" → You use tool...\n",
    "    - Example 2: ...\n",
    "    - Now answer: {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Replace the above with your actual prompt\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test your prompt\n",
    "test_prompt = create_comparison_agent_prompt(\"Compare autumn and winter campaigns\")\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Extend the Simple Agent\n",
    "\n",
    "Modify the `run_simple_agent_groq` function to:\n",
    "1. Add error handling for tool execution failures\n",
    "2. Add a maximum token limit warning\n",
    "3. Save the conversation history to a log file\n",
    "4. Add support for the new `generate_report` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "def run_enhanced_agent(user_query: str, max_iterations: int = 5, log_file: str = \"agent_log.txt\"):\n",
    "    \"\"\"\n",
    "    Enhanced version of the simple agent with additional features.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the enhanced agent\n",
    "    # Add the following features:\n",
    "    # 1. Try-except blocks for tool execution\n",
    "    # 2. Warning if response is getting too long\n",
    "    # 3. Write conversation to log file\n",
    "    # 4. Support for generate_report tool\n",
    "    \n",
    "    pass  # Remove and implement\n",
    "\n",
    "# Test your enhanced agent\n",
    "# run_enhanced_agent(\"Generate an executive report for campaign 1234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Analyze Agent Behavior\n",
    "\n",
    "Run the simple agent with different queries and analyze its behavior.\n",
    "\n",
    "**Queries to test:**\n",
    "1. \"What is the reach of campaign 1234?\"\n",
    "2. \"Compare campaigns 1234 and 5678\"\n",
    "3. \"Calculate the CPM for campaign 1234 with a budget of 50000 euros\"\n",
    "4. \"Which campaign performed better: 1234 or 5678?\"\n",
    "\n",
    "For each query, note:\n",
    "- How many iterations did the agent take?\n",
    "- Which tools did it use?\n",
    "- Was the final answer correct?\n",
    "- Were there any errors or inefficiencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your analysis here\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the reach of campaign 1234?\",\n",
    "    \"Compare campaigns 1234 and 5678\",\n",
    "    \"Calculate the CPM for campaign 1234 with a budget of 50000 euros\",\n",
    "    \"Which campaign performed better: 1234 or 5678?\"\n",
    "]\n",
    "\n",
    "# TODO: Run each query and document your observations\n",
    "# Create a summary table with your findings\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"Testing query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    # Uncomment to run:\n",
    "    # run_simple_agent_groq(query)\n",
    "    \n",
    "# TODO: Write your analysis below\n",
    "analysis = \"\"\"\n",
    "Query 1 Analysis:\n",
    "- Iterations: ?\n",
    "- Tools used: ?\n",
    "- Correct answer: Yes/No\n",
    "- Observations: ...\n",
    "\n",
    "Query 2 Analysis:\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Exercise: Multi-Step Planning\n",
    "\n",
    "Create an agent that can handle a complex multi-step query:\n",
    "\n",
    "**Query:** \"Compare campaigns 1234 and 5678, calculate the CPM for both, and tell me which one is more cost-efficient.\"\n",
    "\n",
    "This requires:\n",
    "1. Getting data for both campaigns\n",
    "2. Calculating KPIs for both\n",
    "3. Comparing the results\n",
    "4. Providing a recommendation\n",
    "\n",
    "Implement this and observe how the agent plans its actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Exercise: Your solution here\n",
    "\n",
    "complex_query = \"\"\"Compare campaigns 1234 and 5678, calculate the CPM for both \n",
    "                   (assume budget of 50000 for 1234 and 60000 for 5678), \n",
    "                   and tell me which one is more cost-efficient.\"\"\"\n",
    "\n",
    "# TODO: Run the agent with this complex query\n",
    "# Observe and document how it breaks down the task\n",
    "\n",
    "# run_simple_agent_groq(complex_query, max_iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **ReAct Pattern**: How agents alternate between reasoning and acting\n",
    "2. **Tool Design**: How to define and implement tools for agents\n",
    "3. **Agent Implementation**: How to create a simple agent using LLMs\n",
    "4. **Practical Application**: How to apply these concepts to the TTVAM use case\n",
    "\n",
    "**Next Steps:**\n",
    "- Complete all exercises\n",
    "- Experiment with different prompts\n",
    "- Try adding new tools\n",
    "- Test edge cases and error scenarios\n",
    "\n",
    "**Preparation for Lesson 4:**\n",
    "- Install Langchain: `pip install langchain langchain-groq langchain-google-genai`\n",
    "- Review the Langchain documentation: https://python.langchain.com/docs/\n",
    "- Think about how frameworks like Langchain can simplify agent development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
