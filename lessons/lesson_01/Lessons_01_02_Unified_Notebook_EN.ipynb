{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c267eaafc84013a0",
   "metadata": {},
   "source": [
    "# Lessons 1 & 2: Large Language Models — Condensed Practical Notebook (EN)\n",
    "## Quick intro to LLMs, basic Transformer ideas, and hands-on usage with Groq and Google AI Studio\n",
    "\n",
    "Course: Building AI Agentic Systems for Advertising Campaign Analysis with LangChain\n",
    "\n",
    "Version: 2.0 (October 2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents (Condensed)\n",
    "\n",
    "This notebook focuses on doing, not theory. For full theory and slides, see:\n",
    "- Lesson 1 (EN): ../lesson_01/Lesson 1 EN.md\n",
    "- Lesson 2 (EN): ../lesson_02/Lesson 2 EN.md\n",
    "\n",
    "What you’ll do here:\n",
    "1. Minimal LLM background (one-page summary)\n",
    "2. API setup for Groq and Gemini (Google AI Studio)\n",
    "3. Hello LLM: first calls\n",
    "4. Generation parameters: temperature and max tokens\n",
    "5. Prompt patterns: instruction, few-shot, CoT\n",
    "6. Hallucination awareness and mitigation basics\n",
    "7. Output validation patterns\n",
    "8. Mini-exercises (8 tasks) with solutions hinted\n",
    "\n",
    "Estimated time: 2–3 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fe73d21de9b24",
   "metadata": {},
   "source": [
    "# Part 1 — What is an LLM (very short)\n",
    "\n",
    "Large Language Models (LLMs) are neural networks trained on diverse text to predict the next token. They can summarize, translate, answer questions, write code, and more. Modern LLMs are built on the Transformer architecture (self-attention) and are often fine-tuned or instructed to follow user prompts.\n",
    "\n",
    "Key ideas to remember:\n",
    "- Context window: how many tokens the model can consider at once.\n",
    "- Temperature: higher → more creative/variable; lower → more deterministic.\n",
    "- System + user prompts: structure and clarity matter more than fancy wording.\n",
    "- Validation: treat model outputs as drafts; verify facts and numbers.\n",
    "\n",
    "For deeper theory: see Lesson 1 EN.md and Lesson 2 EN.md.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a38cbae602f209",
   "metadata": {},
   "source": [
    "# Part 2 — API Setup (Groq + Gemini)\n",
    "\n",
    "In this notebook we’ll use these providers and models:\n",
    "- Groq (OpenAI-compatible API): model `gpt-oss-20b`\n",
    "  - Console: https://console.groq.com\n",
    "  - Docs: https://groq.com\n",
    "- Google AI Studio (Gemini): model `gemini-2.5-flash`\n",
    "  - Console/API keys: https://aistudio.google.com\n",
    "  - Python SDK: `google-generativeai`\n",
    "\n",
    "Notes on keys:\n",
    "- Put your keys into `GROQ_API_KEY` and `GEMINI_API_KEY` (either inline or as environment variables). If not set, calls will simply fail with a clear error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d131d7eef2923e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Install the minimal libraries used in this notebook\n",
    "# You can skip this cell if your environment already has them.\n",
    "# !pip install openai google-generativeai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58866c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: openai in /Users/mattia/.pyenv/versions/2.7.18/lib/python2.7/site-packages (0.6.4)\n",
      "Requirement already satisfied: python-dotenv in /Users/mattia/.pyenv/versions/2.7.18/lib/python2.7/site-packages (0.18.0)\n",
      "Collecting google-generativeai\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement google-generativeai (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for google-generativeai\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai python-dotenv google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234a2676b531c52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Groq uses the OpenAI-compatible SDK\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#import google.generativeai as genai\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Option A (not recommended for shared code): paste keys directly\u001b[39;00m\n\u001b[32m      9\u001b[39m GROQ_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgsk_Vewg7ZKslsGZDrQ9CZLeWGdyb3FY9FIg4nFsd5Y2bnvUtInuvZDa\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "# Groq uses the OpenAI-compatible SDK\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Option A (not recommended for shared code): paste keys directly\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Note: GROQ_API_KEY is not set. Set it to call Groq.\")\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"Note: GEMINI_API_KEY is not set. Set it to call Gemini.\")\n",
    "\n",
    "# Configure clients if keys exist (won't crash if missing; we'll handle errors on call)\n",
    "_groq_client: Any = None\n",
    "_gemini_ready: bool = False\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    _groq_client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    _gemini_ready = True\n",
    "\n",
    "print(\"Clients prepared (if keys are set).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ef88df3322e09",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "- `call_groq` — uses Groq’s OpenAI-compatible endpoint (model: `gpt-oss-20b`)\n",
    "- `call_gemini` — uses Google’s Gemini SDK (model: `gemini-2.5-flash`)\n",
    "- `call_llm` — unified interface: `provider` in {`groq`, `gemini`}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c9f3c730ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def call_groq(prompt: str,\n",
    "              temperature: float = 0.0,\n",
    "              max_tokens: int = 800,\n",
    "              model: str = \"gpt-oss-20b\") -> str:\n",
    "    \"\"\"Call Groq via OpenAI-compatible API.\n",
    "\n",
    "    Args:\n",
    "        prompt: User prompt.\n",
    "        temperature: 0.0–2.0. Lower = more deterministic.\n",
    "        max_tokens: Max tokens in the response.\n",
    "        model: Groq model name.\n",
    "    Returns:\n",
    "        Model response text or an error message.\n",
    "    \"\"\"\n",
    "    if _groq_client is None:\n",
    "        return \"Groq client not configured. Set GROQ_API_KEY and re-run.\"\n",
    "    try:\n",
    "        response = _groq_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Groq call failed: {e}\"\n",
    "\n",
    "\n",
    "def call_gemini(prompt: str,\n",
    "                temperature: float = 0.0,\n",
    "                max_tokens: int = 800,\n",
    "                model: str = \"gemini-2.5-flash\") -> str:\n",
    "    \"\"\"Call Google Gemini via google-generativeai SDK.\n",
    "\n",
    "    Args:\n",
    "        prompt: User prompt.\n",
    "        temperature: 0.0–2.0.\n",
    "        max_tokens: Max output tokens.\n",
    "        model: Gemini model name.\n",
    "    Returns:\n",
    "        Model response text or an error message.\n",
    "    \"\"\"\n",
    "    if not _gemini_ready:\n",
    "        return \"Gemini not configured. Set GEMINI_API_KEY and re-run.\"\n",
    "    try:\n",
    "        model_instance = genai.GenerativeModel(model)\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_tokens,\n",
    "        )\n",
    "        response = model_instance.generate_content(prompt, generation_config=generation_config)\n",
    "        return (response.text or \"\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"Gemini call failed: {e}\"\n",
    "\n",
    "\n",
    "def call_llm(prompt: str,\n",
    "             provider: str = \"groq\",\n",
    "             temperature: float = 0.0,\n",
    "             max_tokens: int = 800,\n",
    "             model: Optional[str] = None,\n",
    "             verbose: bool = True) -> str:\n",
    "    \"\"\"Unified interface to call Groq or Gemini.\"\"\"\n",
    "    provider_norm = provider.lower().strip()\n",
    "\n",
    "    if provider_norm == \"groq\":\n",
    "        model = model or \"gpt-oss-20b\"\n",
    "        if verbose:\n",
    "            print(f\"[Groq] model={model} temperature={temperature} max_tokens={max_tokens}\")\n",
    "        return call_groq(prompt, temperature=temperature, max_tokens=max_tokens, model=model)\n",
    "\n",
    "    if provider_norm == \"gemini\":\n",
    "        model = model or \"gemini-2.5-flash\"\n",
    "        if verbose:\n",
    "            print(f\"[Gemini] model={model} temperature={temperature} max_tokens={max_tokens}\")\n",
    "        return call_gemini(prompt, temperature=temperature, max_tokens=max_tokens, model=model)\n",
    "\n",
    "    return \"Unknown provider. Use 'groq' or 'gemini'.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25921ce17eb77785",
   "metadata": {},
   "source": [
    "# Part 3 — First calls\n",
    "\n",
    "Run a simple prompt on both providers to verify your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0368749521e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Summarize in one sentence why validation is important when using LLMs in analytics.\"\n",
    "\n",
    "print(\"== Groq ==\")\n",
    "print(call_llm(prompt, provider=\"groq\", verbose=False))\n",
    "\n",
    "print(\"\\n== Gemini ==\")\n",
    "print(call_llm(prompt, provider=\"gemini\", verbose=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a36e71e68ada56",
   "metadata": {},
   "source": [
    "# Part 4 — Generation parameters\n",
    "\n",
    "Two important knobs:\n",
    "- Temperature controls randomness.\n",
    "- Max tokens caps the length of the answer.\n",
    "\n",
    "Try different temperatures and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283a0c85a4cd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give three diverse brainstorming ideas for a TV ad about eco-friendly travel, one line each.\"\n",
    "for temp in [0.0, 0.3, 0.7, 0.9]:\n",
    "    print(f\"\\n--- Temperature={temp} (Groq) ---\")\n",
    "    print(call_llm(question, provider=\"groq\", temperature=temp, max_tokens=150, verbose=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a46993",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (2816687793.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\"\"\"\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"You are an expert {role}. You produce accurate, structured and reliable outputs.\n",
    "Your task is to {task}.\n",
    "Focus on: {priorità}.\n",
    "\n",
    "CONTEXT:\n",
    "- {contesto 1}\n",
    "- {contesto 2}\n",
    "- {contesto 3}\n",
    "\n",
    "RULES:\n",
    "- Style: {stile}\n",
    "- Format: {formato}\n",
    "- Length: {lunghezza}\n",
    "- Avoid: {cose da evitare}\n",
    "\n",
    "EXAMPLE (optional):\n",
    "{eventuale esempio}\n",
    "\n",
    "OUTPUT:\n",
    "Respond using the following structure:\n",
    "{struttura di output desiderata}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd1a8174e8bbe",
   "metadata": {},
   "source": [
    "# Part 5 — Prompt patterns\n",
    "\n",
    "We’ll practice three common patterns: instruction, few-shot, and chain-of-thought (CoT).\n",
    "\n",
    "Note: Avoid revealing CoT in production outputs. Use CoT internally or request explanations only when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ef394432cf25",
   "metadata": {},
   "source": [
    "### 5.1 Instruction prompt\n",
    "\n",
    "Keep it short, specify format, and define constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea005b8e76e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = (\n",
    "    \"You are a helpful marketing analyst.\\n\"\n",
    "    \"Task: Write a 50-word summary about the target audience A25-54 for a sports streaming service.\\n\"\n",
    "    \"Constraints: Use plain English; include 1 key behavior and 1 pain point.\"\n",
    ")\n",
    "print(call_llm(inst, provider=\"groq\", verbose=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce80eea8bff05c6",
   "metadata": {},
   "source": [
    "### 5.2 Few-shot prompt\n",
    "\n",
    "Provide 1–3 examples to nudge the model toward your preferred style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d935554ce58d8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'call_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m few_shot = (\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRewrite the claim in a concise, benefit-led style.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExample:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInput: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mOur battery lasts 20 hours so you can keep going all day.\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOutput: Example 1: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m[Immagine 1, Immagine 2, Immagine5]\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNow you try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInput: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mOur platform lets marketers analyze TV campaigns faster.\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcall_llm\u001b[49m(few_shot, provider=\u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'call_llm' is not defined"
     ]
    }
   ],
   "source": [
    "few_shot = (\n",
    "    \"\"\"Dammi la durata potenziale di questa batteria e dimmi per quale finalità la usi. Analizzami il documento dell'utilizzo della batteria e calcolami la durata come chiesto.\n",
    "    Output:\n",
    "    - Example 1: '{\"tempo\": 3h, \"task\": luce accesa}'\n",
    "    - Example 1: '{\"tempo\": 2h, \"task\":lavatrice in standby}'\n",
    "    - Example 1: '{\"tempo\": 10h, \"task\": led acceso}'\"\"\"\n",
    ")\n",
    "print(call_llm(few_shot, provider=\"gemini\", verbose=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19197aa2874d904c",
   "metadata": {},
   "source": [
    "### 5.3 Chain-of-Thought (CoT) vs. direct answer\n",
    "\n",
    "First, ask for an answer without reasoning; then ask for a step-by-step explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5cce1d8cc1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot = (\n",
    "    \"Classify the statement as positive, neutral, or negative: 'The campaign reached 45% but frequency was only 1.1.'\\n\"\n",
    "    \"Answer with one word.\"\n",
    ")\n",
    "with_cot = (\n",
    "    \"Classify the statement as positive, neutral, or negative: 'The campaign reached 45% but frequency was only 1.1.'\\n\"\n",
    "    \"Explain your reasoning briefly, then give the final label.\"\n",
    ")\n",
    "print(\"No-CoT:\\n\", call_llm(no_cot, provider=\"groq\", verbose=False))\n",
    "print(\"\\nWith CoT:\\n\", call_llm(with_cot, provider=\"groq\", verbose=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771929ac2004d11",
   "metadata": {},
   "source": [
    "# Part 6 — Hallucination awareness (lightweight)\n",
    "\n",
    "LLMs can produce confident but incorrect answers. Tactics:\n",
    "- Ask for sources or confidence ranges (still not guarantees).\n",
    "- Provide context within the prompt (grounding).\n",
    "- Keep tasks constrained and verifiable.\n",
    "\n",
    "Try a grounded prompt by giving the model the relevant facts first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c2e275309158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = (\n",
    "    \"Facts:\\n- Budget: €500k\\n- Channels: TV + CTV\\n- Goal: Increase A25-54 reach to 55% with frequency ≥ 3\\n\\n\"\n",
    "    \"Question: Propose two brief tactics that align with the goal.\"\n",
    ")\n",
    "print(call_llm(context, provider=\"gemini\", verbose=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d3d563f8e9fbd",
   "metadata": {},
   "source": [
    "# Part 7 — Output validation patterns\n",
    "\n",
    "Examples:\n",
    "- Ask the model to output JSON that you can parse and validate.\n",
    "- Add simple schema rules in the prompt.\n",
    "- Post-validate in Python; if it fails, ask the model to correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2332868bd547d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prompt = (\n",
    "    \"Return a JSON object with keys 'channel', 'reach_target', 'confidence' and 'risk_note'.\\n\"\n",
    "    \"Constraints: 'reach_target' must be an integer (40–80). Use double quotes.\"\n",
    ")\n",
    "resp = call_llm(validation_prompt, provider=\"groq\", temperature=0.2, verbose=False)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a805c42ce427a5",
   "metadata": {},
   "source": [
    "# Part 8 — Mini-exercises (8)\n",
    "\n",
    "Instructions: try on both providers (Groq and Gemini) when possible.\n",
    "\n",
    "1) Temperature sweep: generate 5 headline variants for a sustainability campaign; compare 0.0 vs 1.0.\n",
    "2) Format control: ask for a 3-row Markdown table with columns: Metric | Definition | Why it matters.\n",
    "3) Style mimic: provide one example claim and ask the model to mimic the style for a new product.\n",
    "4) Guarded claims: give budget and reach goals; ask the model to propose 3 tactics and include 1 risk per tactic.\n",
    "5) CoT: ask for a step-by-step short reasoning to classify performance as good/ok/poor.\n",
    "6) JSON output: request a schema and validate keys in Python; if invalid, ask the model to fix.\n",
    "7) Prompt repair: give a poor prompt; ask the model to rewrite it to be precise and testable.\n",
    "8) Provider compare: run the same prompt on Groq and Gemini and note differences in style/latency.\n",
    "\n",
    "Solutions: see comments and hints in each previous section; adapt as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80952242c670fc6",
   "metadata": {},
   "source": [
    "# Appendix — Useful links\n",
    "\n",
    "- Groq: https://groq.com — Console: https://console.groq.com\n",
    "- Google AI Studio (Gemini): https://aistudio.google.com — Python SDK: https://github.com/google-gemini/generative-ai-python\n",
    "- Prompting basics: https://platform.openai.com/docs/guides/prompt-engineering (general concepts)\n",
    "\n",
    "End of condensed notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
